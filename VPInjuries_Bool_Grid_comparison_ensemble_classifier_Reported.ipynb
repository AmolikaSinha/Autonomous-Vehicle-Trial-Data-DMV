{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Comparison of ensembling classifiers internally using sampling\n",
    "\n",
    "\n",
    "Ensembling classifiers have shown to improve classification performance compare\n",
    "to single learner. However, they will be affected by class imbalance. This\n",
    "example shows the benefit of balancing the training set before to learn\n",
    "learners. We are making the comparison with non-balanced ensemble methods.\n",
    "\n",
    "We make a comparison using the balanced accuracy and geometric mean which are\n",
    "metrics widely used in the literature to evaluate models learned on imbalanced\n",
    "set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n",
    "# License: MIT\n",
    "\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score warning\n",
    "When true positive + false positive == 0, precision is undefined; When true positive + false negative == 0, recall is undefined. In such cases, by default the metric will be set to 0, as will f-score, and UndefinedMetricWarning will be raised. This behavior can be modified with zero_division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, ax,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    print(cm)\n",
    "    print('')\n",
    "\n",
    "    ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.sca(ax)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, format(cm[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing our favorite libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load an imbalanced dataset\n",
    "##############################################################################\n",
    " We will load the UCI SatImage dataset which has an imbalanced ratio of 9.3:1\n",
    " (number of majority sample for a minority sample). The data are then split\n",
    " into training and testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\vince\\\\Desktop\\\\Jupiter\\\\DMV_Crash_Data_Bool.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8253290d3f4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\vince\\Desktop\\Jupiter\\DMV_Crash_Data_Bool.xlsx'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_header\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, engine)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xlrd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xlrd\\__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# a ZIP file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\vince\\\\Desktop\\\\Jupiter\\\\DMV_Crash_Data_Bool.xlsx'"
     ]
    }
   ],
   "source": [
    "df_p = pd.read_excel (r'C:\\Users\\vince\\Desktop\\Jupiter\\DMV_Crash_Data_Bool.xlsx', skip_header = True)\n",
    "print(df_p.head())\n",
    "print((df_p).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = (df_p[\"vehicle type\"], df_p[\"Number for Road Type\"], df_p[\"Intersection\"],\n",
    "    df_p[\"Intersection Geometry\"], df_p[\"Parking provision\"], df_p[\"Mode\"],\n",
    "    df_p[\"Vehicle1 Status\"], df_p[\"Fault\"], df_p[\"NO. for collision type\"],\n",
    "    df_p[\"InjuriesBool\"], df_p[\"Vehicle1 Damage\"], df_p[\"Vehicle2 Damage\"], df_p[\"signal\"])\n",
    "df_p = pd.DataFrame(data = filtered_df)\n",
    "print(type(df_p))\n",
    "print(df_p.head())\n",
    "df_p = df_p.transpose()\n",
    "print(type(df_p))\n",
    "#print(df_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Na values\n",
    "\n",
    "df_R = df_p.dropna(axis=0) #axis=0 \n",
    "#Determine if rows or columns which contain missing values are removed.\n",
    "#0, or ‘index’ : Drop rows which contain missing values.\n",
    "print((df_R).shape)\n",
    "print(df_R.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = (df_R[\"vehicle type\"], df_R[\"Number for Road Type\"], df_R[\"Intersection\"],\n",
    "    df_R[\"Intersection Geometry\"], df_R[\"Parking provision\"], df_R[\"Mode\"],\n",
    "    df_R[\"Vehicle1 Status\"], df_R[\"Fault\"], df_R[\"NO. for collision type\"],\n",
    "    df_R[\"Vehicle1 Damage\"], df_R[\"Vehicle2 Damage\"], df_R[\"signal\"])\n",
    "print(type(X_df))\n",
    "print(len(X_df))\n",
    "X_np= np.asarray(X_df)\n",
    "print(type(X_np))\n",
    "print((X_np).shape)\n",
    "X_np= np.transpose(X_df)\n",
    "print(type(X_np))\n",
    "print((X_np).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please enter the dependent variables\n",
    "Y_df = (df_R[\"InjuriesBool\"])\n",
    "print(type(Y_df))\n",
    "print(len(Y_df))\n",
    "Y_np= np.asarray(Y_df)\n",
    "print(type(Y_np))\n",
    "print((Y_np).shape)\n",
    "Y_np= np.transpose(Y_df)\n",
    "print(type(Y_np))\n",
    "print((Y_np).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_np, Y_np, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=42, stratify = Y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_class = [-1,  1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding class weight\n",
    "class_weight = int(y_train.value_counts()[0]/y_train.value_counts()[1])\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "satimage = fetch_datasets()['satimage']\n",
    "X, y = satimage.data, satimage.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification using a single decision tree\n",
    "##############################################################################\n",
    " We train a decision tree classifier which will be used as a baseline for the\n",
    " rest of this example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are reported in terms of balanced accuracy and geometric mean\n",
    "which are metrics widely used in the literature to validate model trained on\n",
    "imbalanced set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "print('Decision tree classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_tree),\n",
    "              geometric_mean_score(y_test, y_pred_tree)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_tree)))\n",
    "cm_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "fig, ax = plt.subplots()\n",
    "plot_confusion_matrix(cm_tree, classes=my_class, ax=ax,\n",
    "                      title='Decision tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_forest = DecisionTreeClassifier() #defining type of model _ aim to find the best parameters\n",
    "\n",
    "params = { #\"class_weight\": [{0:1,1:class_weight}, {0:1,1:8}, {0:1,1:10}],\n",
    "          #\"n_estimators\": [40, 100,300], \n",
    "          \"max_features\": [\"sqrt\", \"log2\", \"auto\"]\n",
    "          #\"max_depth\": [5,8,15], \n",
    "          #\"min_samples_leaf\" : [1, 2, 4],\n",
    "\n",
    "#\"bootstrap\": [True, False],\n",
    "\n",
    "#\"ccp_alpha\": [0.0, 1.0],\n",
    "\n",
    "#\"criterion\": ['mse', 'mae'], \"max_depth\" : [5, 8, 15], \"max_features\" :['auto', 'sqrt', 'log2', 2],\n",
    "\n",
    "#\"max_leaf_nodes\" = None,\n",
    "\n",
    "#\"max_samples\" = None,\n",
    "\n",
    "#\"min_impurity_decrease\"0.0,\n",
    "\n",
    "#\"min_impurity_split\":None, \"min_samples_leaf\": [1, 2, 3, 4, 5], \"min_samples_split\": [2,3],\n",
    "\n",
    "#\"min_weight_fraction_leaf\" : 0.0, \"n_jobs\": [4,-1],\n",
    "\n",
    "#n_estimators=100, n_jobs=None,\n",
    "\n",
    "#oob_score=False,\n",
    "\n",
    "#random_state=None,\n",
    "\n",
    "#verbose=0, warm_start=False \n",
    "         }\n",
    "\n",
    "clf = GridSearchCV(gridsearch_forest, param_grid=params, cv=5 ) \n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_features = \"sqrt\")  #INSERT THE BEST PARAMETER\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "print('Decision tree classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_tree),\n",
    "              geometric_mean_score(y_test, y_pred_tree)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_tree)))\n",
    "cm_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "fig, ax = plt.subplots()\n",
    "plot_confusion_matrix(cm_tree, classes=my_class, ax=ax,\n",
    "                      title='Decision tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier with Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(class_weight= {0:1,1:class_weight})\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "print('Decision tree classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_tree),\n",
    "              geometric_mean_score(y_test, y_pred_tree)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_tree)))\n",
    "cm_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "fig, ax = plt.subplots()\n",
    "plot_confusion_matrix(cm_tree, classes=my_class, ax=ax,\n",
    "                      title='Decision tree with Weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_forest = DecisionTreeClassifier()\n",
    "\n",
    "params = { \"class_weight\": [{0:1,1:class_weight}, {0:1,1:8}, {0:1,1:10}],\n",
    "          #\"n_estimators\": [10, 25, 50, 75],\n",
    "          #\"n_estimators\": [40, 100,300], \n",
    "          \"max_features\": [\"sqrt\", \"log2\", \"auto\"]\n",
    "          #\"max_depth\": [5,8,15], \n",
    "          #\"min_samples_leaf\" : [1, 2, 4],\n",
    "\n",
    "#\"bootstrap\": [True, False],\n",
    "\n",
    "#\"ccp_alpha\": [0.0, 1.0],\n",
    "\n",
    "#\"criterion\": ['mse', 'mae'], \"max_depth\" : [5, 8, 15], \"max_features\" :['auto', 'sqrt', 'log2', 2],\n",
    "\n",
    "#\"max_leaf_nodes\" = None,\n",
    "\n",
    "#\"max_samples\" = None,\n",
    "\n",
    "#\"min_impurity_decrease\"0.0,\n",
    "\n",
    "#\"min_impurity_split\":None, \"min_samples_leaf\": [1, 2, 3, 4, 5], \"min_samples_split\": [2,3],\n",
    "\n",
    "#\"min_weight_fraction_leaf\" : 0.0, \"n_jobs\": [4,-1],\n",
    "\n",
    "#n_estimators=100, n_jobs=None,\n",
    "\n",
    "#oob_score=False,\n",
    "\n",
    "#random_state=None,\n",
    "\n",
    "#verbose=0, warm_start=False \n",
    "         }\n",
    "\n",
    "clf = GridSearchCV(gridsearch_forest, param_grid=params, cv=5 ) \n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(class_weight= {0:1,1:8},max_features = \"log2\")  #Change the class weight and include the best paramter\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "print('Decision tree classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_tree),\n",
    "              geometric_mean_score(y_test, y_pred_tree)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_tree)))\n",
    "cm_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "fig, ax = plt.subplots()\n",
    "plot_confusion_matrix(cm_tree, classes=my_class, ax=ax,\n",
    "                      title='Decision tree with Weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging \n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification using bagging classifier with and without sampling\n",
    "##############################################################################\n",
    " Instead of using a single tree, we will check if an ensemble of decsion tree\n",
    " can actually alleviate the issue induced by the class imbalancing. First, we\n",
    " will use a bagging classifier and its counter part which internally uses a\n",
    " random under-sampling to balanced each boostrap sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging = BaggingClassifier(n_estimators=50, random_state=0)\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bc = bagging.predict(X_test)\n",
    "\n",
    "\n",
    "print('Bagging classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_bc),\n",
    "              geometric_mean_score(y_test, y_pred_bc)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_bc)))\n",
    "cm_bagging = confusion_matrix(y_test, y_pred_bc)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_bagging, classes=my_class, ax=ax,\n",
    "                      title='Bagging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing each bootstrap sample allows to increase significantly the balanced\n",
    "accuracy and the geometric mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Bagging classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_bc),\n",
    "              geometric_mean_score(y_test, y_pred_bc)))\n",
    "cm_bagging = confusion_matrix(y_test, y_pred_bc)\n",
    "fig, ax = plt.subplots(ncols=2)\n",
    "plot_confusion_matrix(cm_bagging, classes=my_class, ax=ax[0],\n",
    "                      title='Bagging')\n",
    "\n",
    "print('Balanced Bagging classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_bbc),\n",
    "              geometric_mean_score(y_test, y_pred_bbc)))\n",
    "cm_balanced_bagging = confusion_matrix(y_test, y_pred_bbc)\n",
    "plot_confusion_matrix(cm_balanced_bagging, classes=my_class,\n",
    "                      ax=ax[1], title='Balanced bagging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_forest = BaggingClassifier()\n",
    "\n",
    "params = { #\"class_weight\": [{0:1,1:class_weight}, {0:1,1:8}, {0:1,1:10}],\n",
    "          \"n_estimators\": [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100], \n",
    "          #\"max_features\": [\"sqrt\", \"log2\", \"None\"],\n",
    "          \"oob_score\":['False', 'True']\n",
    "          #\"max_depth\": [5,8,15], \n",
    "          #\"min_samples_leaf\" : [1, 2, 4],\n",
    "\n",
    "#\"bootstrap\": [True, False],\n",
    "\n",
    "#\"ccp_alpha\": [0.0, 1.0],\n",
    "\n",
    "#\"criterion\": ['mse', 'mae'], \"max_depth\" : [5, 8, 15], \"max_features\" :['auto', 'sqrt', 'log2', 2],\n",
    "\n",
    "#\"max_leaf_nodes\" = None,\n",
    "\n",
    "#\"max_samples\" = None,\n",
    "\n",
    "#\"min_impurity_decrease\"0.0,\n",
    "\n",
    "#\"min_impurity_split\":None, \"min_samples_leaf\": [1, 2, 3, 4, 5], \"min_samples_split\": [2,3],\n",
    "\n",
    "#\"min_weight_fraction_leaf\" : 0.0, \"n_jobs\": [4,-1],\n",
    "\n",
    "#n_estimators=100, n_jobs=None,\n",
    "\n",
    "            \n",
    "\n",
    "#random_state=None,\n",
    "\n",
    "#verbose=0, warm_start=False \n",
    "         }\n",
    "\n",
    "clf = GridSearchCV(gridsearch_forest, param_grid=params, cv=5 ) \n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging = BaggingClassifier(n_estimators=30, random_state=0) #choose the parameter and keep random_state=0\n",
    "#random state default=None\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bc = bagging.predict(X_test)\n",
    "\n",
    "\n",
    "print('Bagging classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_bc),\n",
    "              geometric_mean_score(y_test, y_pred_bc)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_bc)))\n",
    "cm_bagging = confusion_matrix(y_test, y_pred_bc)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_bagging, classes=my_class, ax=ax,\n",
    "                      title='Bagging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced Bagging\n",
    "(https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.ensemble.BalancedBaggingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_bagging = BalancedBaggingClassifier(n_estimators=50, random_state=0)\n",
    "balanced_bagging.fit(X_train, y_train)\n",
    "y_pred_bbc = balanced_bagging.predict(X_test)\n",
    "print('Balanced Bagging classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_bbc),\n",
    "              geometric_mean_score(y_test, y_pred_bbc)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_bbc)))\n",
    "cm_balanced_bagging = confusion_matrix(y_test, y_pred_bbc)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_balanced_bagging, classes=my_class,\n",
    "                      ax=ax, title='Balanced bagging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_forest = BalancedBaggingClassifier()\n",
    "\n",
    "params = { #\"class_weight\": [{0:1,1:class_weight}, {0:1,1:8}, {0:1,1:10}],\n",
    "          \"n_estimators\": [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100], \n",
    "          #\"max_features\": [\"sqrt\", \"log2\", \"auto\"],\n",
    "          \"oob_score\": ['False', 'True'],\n",
    "          #\"max_depth\": [5,8,15], \n",
    "          #\"min_samples_leaf\" : [1, 2, 4],\n",
    "\n",
    "#\"bootstrap\": [True, False],\n",
    "\n",
    "#\"ccp_alpha\": [0.0, 1.0],\n",
    "\n",
    "#\"criterion\": ['mse', 'mae'], \"max_depth\" : [5, 8, 15], \"max_features\" :['auto', 'sqrt', 'log2', 2],\n",
    "\n",
    "#\"max_leaf_nodes\" = None,\n",
    "\n",
    "#\"max_samples\" = None,\n",
    "\n",
    "#\"min_impurity_decrease\"0.0,\n",
    "\n",
    "#\"min_impurity_split\":None, \"min_samples_leaf\": [1, 2, 3, 4, 5], \"min_samples_split\": [2,3],\n",
    "\n",
    "#\"min_weight_fraction_leaf\" : 0.0, \"n_jobs\": [4,-1],\n",
    "\n",
    "#n_estimators=100, n_jobs=None,\n",
    "\n",
    "            \n",
    "\n",
    "#random_state=None,\n",
    "\n",
    "#verbose=0, warm_start=False \n",
    "         }\n",
    "\n",
    "clf = GridSearchCV(gridsearch_forest, param_grid=params, cv=5 ) \n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_bagging = BalancedBaggingClassifier(n_estimators=55, random_state=0) #choose the parameter and keep random_state=0\n",
    "balanced_bagging.fit(X_train, y_train)\n",
    "y_pred_bbc = balanced_bagging.predict(X_test)\n",
    "print('Balanced Bagging classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_bbc),\n",
    "              geometric_mean_score(y_test, y_pred_bbc)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_bbc)))\n",
    "cm_balanced_bagging = confusion_matrix(y_test, y_pred_bbc)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_balanced_bagging, classes=my_class,\n",
    "                      ax=ax, title='Balanced bagging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification using random forest classifier with and without sampling\n",
    "##############################################################################\n",
    " Random forest is another popular ensemble method and it is usually\n",
    " outperforming bagging. Here, we used a vanilla random forest and its balanced\n",
    " counterpart in which each bootstrap sample is balanced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "\n",
    "# Similarly to the previous experiment, the balanced classifier outperform the\n",
    "# classifier which learn from imbalanced bootstrap samples. In addition, random\n",
    "# forest outsperforms the bagging classifier.\n",
    "\n",
    "print('Random Forest classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_rf),\n",
    "              geometric_mean_score(y_test, y_pred_rf)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_rf)))\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_rf, classes=my_class, ax=ax,\n",
    "                      title='Random forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_forest = RandomForestClassifier()\n",
    "\n",
    "params = { \"class_weight\": [{0:1,1:class_weight}, {0:1,1:8}, {0:1,1:10}],\n",
    "          \"n_estimators\": [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100], \n",
    "          \"max_features\": [\"sqrt\", \"log2\", \"auto\"],\n",
    "          \"oob_score\": ['False', 'True'],\n",
    "          #\"max_depth\": [5,8,15], \n",
    "          #\"min_samples_leaf\" : [1, 2, 4],\n",
    "\n",
    "#\"bootstrap\": [True, False],\n",
    "\n",
    "#\"ccp_alpha\": [0.0, 1.0],\n",
    "\n",
    "#\"criterion\": ['mse', 'mae'], \"max_depth\" : [5, 8, 15], \"max_features\" :['auto', 'sqrt', 'log2', 2],\n",
    "\n",
    "#\"max_leaf_nodes\" = None,\n",
    "\n",
    "#\"max_samples\" = None,\n",
    "\n",
    "#\"min_impurity_decrease\"0.0,\n",
    "\n",
    "#\"min_impurity_split\":None, \"min_samples_leaf\": [1, 2, 3, 4, 5], \"min_samples_split\": [2,3],\n",
    "\n",
    "#\"min_weight_fraction_leaf\" : 0.0, \"n_jobs\": [4,-1],\n",
    "\n",
    "#n_estimators=100, n_jobs=None,\n",
    "\n",
    "            \n",
    "\n",
    "#random_state=None,\n",
    "\n",
    "#verbose=0, warm_start=False \n",
    "         }\n",
    "\n",
    "clf = GridSearchCV(gridsearch_forest, param_grid=params, cv=5 ) \n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=20, max_features = \"auto\"\n",
    "                            , class_weight = {0:1,1:5}, random_state=0)  #choose the parameter and keep random_state=0\n",
    "#n_estimators, default=100\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Similarly to the previous experiment, the balanced classifier outperform the\n",
    "# classifier which learn from imbalanced bootstrap samples. In addition, random\n",
    "# forest outsperforms the bagging classifier.\n",
    "\n",
    "print('Random Forest classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_rf),\n",
    "              geometric_mean_score(y_test, y_pred_rf)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_rf)))\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_rf, classes=my_class, ax=ax,\n",
    "                      title='Random forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced Random Forest\n",
    "(https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.ensemble.BalancedRandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brf = BalancedRandomForestClassifier(n_estimators=50, random_state=0, class_weight= {0:1,1:class_weight})\n",
    "brf.fit(X_train, y_train)\n",
    "y_pred_brf = brf.predict(X_test)\n",
    "\n",
    "# Similarly to the previous experiment, the balanced classifier outperform the\n",
    "# classifier which learn from imbalanced bootstrap samples. In addition, random\n",
    "# forest outsperforms the bagging classifier.\n",
    "print('Balanced Random Forest classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_brf),\n",
    "              geometric_mean_score(y_test, y_pred_brf)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_brf)))\n",
    "cm_brf = confusion_matrix(y_test, y_pred_brf)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_brf, classes=my_class, ax=ax,\n",
    "                      title='Balanced random forest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_forest = BalancedRandomForestClassifier()\n",
    "\n",
    "params = {\"class_weight\": [{0:1,1:class_weight}, {0:1,1:8}, {0:1,1:10}],\n",
    "          \"n_estimators\": [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100], \n",
    "          \"max_features\": [\"sqrt\", \"log2\", \"auto\"],\n",
    "          \"oob_score\": ['False', 'True'],\n",
    "          #\"max_depth\": [5,8,15], \n",
    "          #\"min_samples_leaf\" : [1, 2, 4],\n",
    "\n",
    "#\"bootstrap\": [True, False],\n",
    "\n",
    "#\"ccp_alpha\": [0.0, 1.0],\n",
    "\n",
    "#\"criterion\": ['mse', 'mae'], \"max_depth\" : [5, 8, 15], \"max_features\" :['auto', 'sqrt', 'log2', 2],\n",
    "\n",
    "#\"max_leaf_nodes\" = None,\n",
    "\n",
    "#\"max_samples\" = None,\n",
    "\n",
    "#\"min_impurity_decrease\"0.0,\n",
    "\n",
    "#\"min_impurity_split\":None, \"min_samples_leaf\": [1, 2, 3, 4, 5], \"min_samples_split\": [2,3],\n",
    "\n",
    "#\"min_weight_fraction_leaf\" : 0.0, \"n_jobs\": [4,-1],\n",
    "\n",
    "#n_estimators=100, n_jobs=None,\n",
    "\n",
    "            \n",
    "\n",
    "#random_state=None,\n",
    "\n",
    "#verbose=0, warm_start=False \n",
    "         }\n",
    "\n",
    "clf = GridSearchCV(gridsearch_forest, param_grid=params, cv=5 ) \n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brf = BalancedRandomForestClassifier(n_estimators=65, max_features = \"sqrt\",\n",
    "                                     random_state=0, class_weight= {0:1,1:5}) #Use the best parameter and keep random_state =0\n",
    "brf.fit(X_train, y_train)\n",
    "y_pred_brf = brf.predict(X_test)\n",
    "\n",
    "# Similarly to the previous experiment, the balanced classifier outperform the\n",
    "# classifier which learn from imbalanced bootstrap samples. In addition, random\n",
    "# forest outsperforms the bagging classifier.\n",
    "print('Balanced Random Forest classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_brf),\n",
    "              geometric_mean_score(y_test, y_pred_brf)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_brf)))\n",
    "cm_brf = confusion_matrix(y_test, y_pred_brf)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_brf, classes=my_class, ax=ax,\n",
    "                      title='Balanced random forest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Balanced Random Forest classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_brf),\n",
    "              geometric_mean_score(y_test, y_pred_brf)))\n",
    "cm_brf = confusion_matrix(y_test, y_pred_brf)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_brf, classes=my_class, ax=ax,\n",
    "                      title='Balanced random forest')\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=0, class_weight= {0:1,1:class_weight})\n",
    "brf = BalancedRandomForestClassifier(n_estimators=50, random_state=0, class_weight= {0:1,1:class_weight})\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "brf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_brf = brf.predict(X_test)\n",
    "\n",
    "#Similarly to the previous experiment, the balanced classifier outperform the\n",
    "#classifier which learn from imbalanced bootstrap samples. In addition, random\n",
    "#forest outsperforms the bagging classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier with given weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "print('Random Forest classifier performance: with weigths')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_rf),\n",
    "              geometric_mean_score(y_test, y_pred_rf)))\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_rf, classes=my_class, ax=ax,\n",
    "                      title='Random forest with Weigths')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Balanced Random Forest classifier performance: weigths')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_brf),\n",
    "              geometric_mean_score(y_test, y_pred_brf)))\n",
    "cm_brf = confusion_matrix(y_test, y_pred_brf)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_brf, classes=my_class, ax=ax,\n",
    "                      title='Balanced random forest with Weigths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy emsemble classifier \n",
    "(https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.ensemble.EasyEnsembleClassifier.html#imblearn.ensemble.EasyEnsembleClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting classifier\n",
    "##############################################################################\n",
    " In the same manner, easy ensemble classifier is a bag of balanced AdaBoost\n",
    " classifier. However, it will be slower to train than random forest and will\n",
    " achieve worse performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = AdaBoostClassifier(n_estimators=10)\n",
    "eec = EasyEnsembleClassifier(n_estimators=10,\n",
    "                             base_estimator=base_estimator)\n",
    "eec.fit(X_train, y_train)\n",
    "y_pred_eec = eec.predict(X_test)\n",
    "print('Easy ensemble classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_eec),\n",
    "              geometric_mean_score(y_test, y_pred_eec)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_eec)))\n",
    "cm_eec = confusion_matrix(y_test, y_pred_eec)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_eec, classes=my_class, ax=ax,\n",
    "                      title='Easy ensemble classifier')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_forest = EasyEnsembleClassifier()\n",
    "\n",
    "params = {#\"class_weight\": [{0:1,1:class_weight}, {0:1,1:8}, {0:1,1:10}],\n",
    "          \"n_estimators\": [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100], \n",
    "          #\"max_features\": [\"sqrt\", \"log2\", \"None\"],\n",
    "          #\"oob_score\"=['False', 'True'],\n",
    "          #\"max_depth\": [5,8,15], \n",
    "          #\"min_samples_leaf\" : [1, 2, 4],\n",
    "\n",
    "#\"bootstrap\": [True, False],\n",
    "\n",
    "#\"ccp_alpha\": [0.0, 1.0],\n",
    "\n",
    "#\"criterion\": ['mse', 'mae'], \"max_depth\" : [5, 8, 15], \"max_features\" :['auto', 'sqrt', 'log2', 2],\n",
    "\n",
    "#\"max_leaf_nodes\" = None,\n",
    "\n",
    "#\"max_samples\" = None,\n",
    "\n",
    "#\"min_impurity_decrease\"0.0,\n",
    "\n",
    "#\"min_impurity_split\":None, \"min_samples_leaf\": [1, 2, 3, 4, 5], \"min_samples_split\": [2,3],\n",
    "\n",
    "#\"min_weight_fraction_leaf\" : 0.0, \"n_jobs\": [4,-1],\n",
    "\n",
    "#n_estimators=100, n_jobs=None,\n",
    "\n",
    "            \n",
    "\n",
    "#random_state=None,\n",
    "\n",
    "#verbose=0, warm_start=False \n",
    "         }\n",
    "\n",
    "clf = GridSearchCV(gridsearch_forest, param_grid=params, cv=5 ) \n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = AdaBoostClassifier(n_estimators=10)\n",
    "eec = EasyEnsembleClassifier(n_estimators=10,  #change this n_estimater based on above best parameter\n",
    "                             base_estimator=base_estimator)  \n",
    "eec.fit(X_train, y_train)\n",
    "y_pred_eec = eec.predict(X_test)\n",
    "print('Easy ensemble classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_eec),\n",
    "              geometric_mean_score(y_test, y_pred_eec)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_eec)))\n",
    "cm_eec = confusion_matrix(y_test, y_pred_eec)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_eec, classes=my_class, ax=ax,\n",
    "                      title='Easy ensemble classifier')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUSClassifier \n",
    "https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.ensemble.RUSBoostClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rusboost = RUSBoostClassifier(n_estimators=10,\n",
    "                              base_estimator=base_estimator)\n",
    "rusboost.fit(X_train, y_train)\n",
    "y_pred_rusboost = rusboost.predict(X_test)\n",
    "print('RUSBoost classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_rusboost),\n",
    "              geometric_mean_score(y_test, y_pred_rusboost)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_rusboost)))\n",
    "cm_rusboost = confusion_matrix(y_test, y_pred_rusboost)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_rusboost, classes=my_class,\n",
    "                      ax=ax, title='RUSBoost classifier')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_forest = RUSBoostClassifier()\n",
    "\n",
    "params = {#\"class_weight\": [{0:1,1:class_weight}, {0:1,1:8}, {0:1,1:10}],\n",
    "          \"n_estimators\": [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100], \n",
    "          #\"max_features\": [\"sqrt\", \"log2\", \"None\"],\n",
    "          #\"oob_score\"=['False', 'True'],\n",
    "          #\"max_depth\": [5,8,15], \n",
    "          #\"min_samples_leaf\" : [1, 2, 4],\n",
    "\n",
    "#\"bootstrap\": [True, False],\n",
    "\n",
    "#\"ccp_alpha\": [0.0, 1.0],\n",
    "\n",
    "#\"criterion\": ['mse', 'mae'], \"max_depth\" : [5, 8, 15], \"max_features\" :['auto', 'sqrt', 'log2', 2],\n",
    "\n",
    "#\"max_leaf_nodes\" = None,\n",
    "\n",
    "#\"max_samples\" = None,\n",
    "\n",
    "#\"min_impurity_decrease\"0.0,\n",
    "\n",
    "#\"min_impurity_split\":None, \"min_samples_leaf\": [1, 2, 3, 4, 5], \"min_samples_split\": [2,3],\n",
    "\n",
    "#\"min_weight_fraction_leaf\" : 0.0, \"n_jobs\": [4,-1],\n",
    "\n",
    "#n_estimators=100, n_jobs=None,\n",
    "\n",
    "            \n",
    "\n",
    "#random_state=None,\n",
    "\n",
    "#verbose=0, warm_start=False \n",
    "         }\n",
    "\n",
    "clf = GridSearchCV(gridsearch_forest, param_grid=params, cv=5 ) \n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rusboost = RUSBoostClassifier(n_estimators=70,\n",
    "                              base_estimator=base_estimator)  #change this n_estimater based on above best parameter\n",
    "rusboost.fit(X_train, y_train)\n",
    "y_pred_rusboost = rusboost.predict(X_test)\n",
    "print('RUSBoost classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_rusboost),\n",
    "              geometric_mean_score(y_test, y_pred_rusboost)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_rusboost)))\n",
    "cm_rusboost = confusion_matrix(y_test, y_pred_rusboost)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_rusboost, classes=my_class,\n",
    "                      ax=ax, title='RUSBoost classifier')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_estimator = AdaBoostClassifier(n_estimators=10) #class_weight= {0:1,1:class_weight}) doesn't work on base_estimator\n",
    "eec = EasyEnsembleClassifier(n_estimators=10,\n",
    "                             base_estimator=base_estimator) #class_weight= {0:1,1:class_weight} doesn't work on eec\n",
    "eec.fit(X_train, y_train)\n",
    "y_pred_eec = eec.predict(X_test)\n",
    "print('Easy ensemble classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_eec),\n",
    "              geometric_mean_score(y_test, y_pred_eec)))\n",
    "cm_eec = confusion_matrix(y_test, y_pred_eec)\n",
    "fig, ax = plt.subplots(ncols=2)\n",
    "plot_confusion_matrix(cm_eec, classes=my_class, ax=ax[0],\n",
    "                      title='Easy ensemble classifier')\n",
    "\n",
    "rusboost = RUSBoostClassifier(n_estimators=10,\n",
    "                              base_estimator=base_estimator) #class_weight= {0:1,1:class_weight} doesn't work rusboost\n",
    "rusboost.fit(X_train, y_train)\n",
    "y_pred_rusboost = rusboost.predict(X_test)\n",
    "print('RUSBoost classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_rusboost),\n",
    "              geometric_mean_score(y_test, y_pred_rusboost)))\n",
    "cm_rusboost = confusion_matrix(y_test, y_pred_rusboost)\n",
    "plot_confusion_matrix(cm_rusboost, classes=my_class,\n",
    "                      ax=ax[1], title='RUSBoost classifier')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just playing around with color. Don't bother :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot_confusion_matrix(cm, classes, ax,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.YlGnBu):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    print(cm)\n",
    "    print('')\n",
    "\n",
    "    ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.sca(ax)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, format(cm[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#delete this later\n",
    "base_estimator = AdaBoostClassifier(n_estimators=10)\n",
    "eec = EasyEnsembleClassifier(n_estimators=10,\n",
    "                             base_estimator=base_estimator)\n",
    "eec.fit(X_train, y_train)\n",
    "y_pred_eec = eec.predict(X_test)\n",
    "print('Easy ensemble classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_eec),\n",
    "              geometric_mean_score(y_test, y_pred_eec)))\n",
    "cm_eec = confusion_matrix(y_test, y_pred_eec)\n",
    "fig, ax = plt.subplots(ncols=2)\n",
    "plot_confusion_matrix(cm_eec, classes=my_class, ax=ax[0],\n",
    "                      title='Easy ensemble classifier')\n",
    "\n",
    "rusboost = RUSBoostClassifier(n_estimators=10,\n",
    "                              base_estimator=base_estimator)\n",
    "rusboost.fit(X_train, y_train)\n",
    "y_pred_rusboost = rusboost.predict(X_test)\n",
    "print('RUSBoost classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_rusboost),\n",
    "              geometric_mean_score(y_test, y_pred_rusboost)))\n",
    "cm_rusboost = confusion_matrix(y_test, y_pred_rusboost)\n",
    "plot_confusion_matrix(cm_rusboost, classes=my_class,\n",
    "                      ax=ax[1], title='RUSBoost classifier')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "rf = ExtraTreesClassifier(n_estimators=50, random_state=0)\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "\n",
    "# Similarly to the previous experiment, the balanced classifier outperform the\n",
    "# classifier which learn from imbalanced bootstrap samples. In addition, random\n",
    "# forest outsperforms the bagging classifier.\n",
    "\n",
    "print('Extra Trees classifier performance:')\n",
    "print('Balanced accuracy: {:.2f} - Geometric mean {:.2f}'\n",
    "      .format(balanced_accuracy_score(y_test, y_pred_rf),\n",
    "              geometric_mean_score(y_test, y_pred_rf)))\n",
    "print(\"F1 Score: \" + str(f1_score(y_test,y_pred_rf)))\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "plot_confusion_matrix(cm_rf, classes=my_class, ax=ax,\n",
    "                      title='Extra Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
